{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ae14af",
   "metadata": {},
   "source": [
    "# Floating-point Math problems\n",
    "\n",
    "## Why do languages like Python, C, C++, etc. struggle with floating-point math?\n",
    "\n",
    "The issue is not a bug, but a fundamental limitation of how floating-point numbers are represented in binary.\n",
    "\n",
    "### 1. Floating-point numbers ≠ real numbers\n",
    "\n",
    "Computers use finite binary representations to approximate real numbers. But many simple decimal numbers (like `0.1`, `0.01`, `0.3`) cannot be exactly represented in binary — just like 1/3 can't be exactly written in decimal (`0.333...`).\n",
    "\n",
    "### 2. Example: Decimal 0.1 in binary\n",
    "\n",
    "In binary, `0.1` becomes a repeating fraction:\n",
    "\n",
    "$$\n",
    "0.1_{10} = 0.00011001100110011001100…_2\n",
    "$$\n",
    "\n",
    "It repeats forever. But the computer can only store a finite number of bits, so it rounds to the nearest available binary float.\n",
    "\n",
    "So even though you typed 0.1, internally it’s stored more like:\n",
    "\n",
    "```python\n",
    "0.10000000000000000555...\n",
    "```\n",
    "\n",
    "### 3. These small rounding errors add up\n",
    "\n",
    "When you perform math operations (like Bayes' Rule), especially involving multiplication or subtraction of very close values, these rounding errors become visible:\n",
    "\n",
    "```python\n",
    ">>> 0.1 + 0.2\n",
    "0.30000000000000004\n",
    "```\n",
    "\n",
    "### 4. Why doesn’t Python \"just round it off\"?\n",
    "\n",
    "Python does round when printing — most of the time.\n",
    "\n",
    "```python\n",
    ">>> print(0.1 + 0.2)\n",
    "0.3\n",
    "```\n",
    "\n",
    "But:\n",
    "- When inspecting the raw value (`repr()`), you’ll see the full precision.\n",
    "- Sometimes cumulative operations (like in loops or stats) make the error visible even when printing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5982707",
   "metadata": {},
   "source": [
    "## Basic Calculation Example\n",
    "\n",
    "Let’s perform some basic operations with `0.1` and `0.2` to see how floating-point imprecision shows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic floating-point calculation: 0.1 + 0.2\n",
    "result = 0.1 + 0.2\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceedef3",
   "metadata": {},
   "source": [
    "As you can see, the result is `0.30000000000000004` instead of exactly `0.3`. This happens due to how the numbers are represented internally.\n",
    "\n",
    "---\n",
    "\n",
    "## Rounding in Python\n",
    "\n",
    "While floating-point imprecision is unavoidable in most cases, we can use Python's built-in `round()` function to round the result for display purposes. Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the result to 2 decimal places\n",
    "rounded_result = round(result, 2)\n",
    "rounded_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00305148",
   "metadata": {},
   "source": [
    "By rounding the result to 2 decimal places, we get `0.3`, which is the expected result. This is helpful when you're only concerned with **displaying** the result, not with the exact internal representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizing Floating-Point Precision in Calculations\n",
    "\n",
    "Finally, let’s create a plot to visualize how the floating-point error grows over multiple operations. We'll compute the sum of `0.1` added to itself multiple times and see how the error accumulates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4919664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate cumulative sum of 0.1 added 1000 times\n",
    "errors = []\n",
    "sum_value = 0\n",
    "for i in range(1000):\n",
    "    sum_value += 0.1\n",
    "    errors.append(abs(sum_value - (i + 1) * 0.1))  # Error compared to the expected value\n",
    "\n",
    "# Plot the error\n",
    "plt.plot(errors)\n",
    "plt.title('Accumulating Error in Floating-Point Operations')\n",
    "plt.xlabel('Number of Operations')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3379f",
   "metadata": {},
   "source": [
    "In the plot, you can see how the floating-point error grows with each addition of `0.1`. Even though `0.1` is a simple number, its representation in binary causes small errors to accumulate over many operations.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- Floating-point imprecision is a result of how numbers are stored in binary.\n",
    "- For simple operations, the error might be negligible, but for more complex calculations (like Bayes' Rule or statistical models), the error can accumulate.\n",
    "- Use rounding for display purposes, the `decimal` module for high precision, and `math.isclose()` for comparisons.\n",
    "\n",
    "This is just the beginning of understanding floating-point arithmetic and how it affects computational models. We’ll continue exploring how to mitigate these issues in more advanced algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
